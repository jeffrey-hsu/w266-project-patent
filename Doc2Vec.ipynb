{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import gensim\n",
    "import csv2bow\n",
    "reload(csv2bow)\n",
    "\n",
    "# other utilities\n",
    "import getpass\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load patent file and build tagged training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the file path for patent file, dictionary and corpus\n",
    "current_user = getpass.getuser()\n",
    "base_file_path = '/home/' + current_user + '/'\n",
    "patent_file_path = ''.join((base_file_path, 'patent_data/patent_claims_fulltext.csv'))\n",
    "dictionary_path = ''.join((base_file_path, 'patent_data/dictionary.dict'))\n",
    "corpus_path = ''.join((base_file_path, '/patent_data/corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_test_train_corpus(filepath, tokens_only=False):\n",
    "    if tokens_only == True:\n",
    "        yield [csv2bow.prune(doc[1]) for doc in csv2bow.clump(filepath)]\n",
    "    else:\n",
    "        \n",
    "        yield (gensim.models.doc2vec.TaggedDocument(\n",
    "            csv2bow.prune(doc), [i]) for i, doc in csv2bow.clump(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_corpus = create_test_train_corpus(patent_file_path, tokens_only=False).next()\n",
    "# load same corpus without patent id for evaluations\n",
    "test_corpus = create_test_train_corpus(patent_file_path, tokens_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_corpus = create_test_train_corpus(patent_file_path, tokens_only=False).next()\n",
    "#for i, doc in enumerate(training_corpus):\n",
    "#    if i < 2:\n",
    "#        print doc[1][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 22:18:28,270 : INFO : collecting all words and their counts\n",
      "2017-12-19 22:18:28,326 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-12-19 22:48:49,890 : INFO : collected 20768 word types and 10000 unique tags from a corpus of 10000 examples and 4049558 words\n",
      "2017-12-19 22:48:49,892 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 22:48:49,940 : INFO : min_count=2 retains 14181 unique words (68% of original 20768, drops 6587)\n",
      "2017-12-19 22:48:49,942 : INFO : min_count=2 leaves 4042971 word corpus (99% of original 4049558, drops 6587)\n",
      "2017-12-19 22:48:49,984 : INFO : deleting the raw counts dictionary of 20768 items\n",
      "2017-12-19 22:48:49,988 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-12-19 22:48:49,990 : INFO : downsampling leaves estimated 3260464 word corpus (80.6% of prior 4042971)\n",
      "2017-12-19 22:48:49,992 : INFO : estimated required memory for 14181 words and 160 dimensions: 33642180 bytes\n",
      "2017-12-19 22:48:50,063 : INFO : resetting layer weights\n",
      "2017-12-19 22:48:50,419 : INFO : training model with 3 workers on 14181 vocabulary and 160 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 22:48:50,422 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2017-12-19 22:48:50,424 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 22:48:50,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 22:48:50,428 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 22:48:50,428 : INFO : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-12-19 22:48:50,429 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-19 22:48:50,431 : WARNING : supplied example count (0) did not equal expected count (500000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 47s, sys: 31.9 s, total: 15min 19s\n",
      "Wall time: 30min 22s\n"
     ]
    }
   ],
   "source": [
    "%time mod_doc2vec = gensim.models.doc2vec.Doc2Vec(training_corpus, size=160, min_count=2, iter=50)\n",
    "## Uncomment the following lines if no corpus list or corpus generator is given to initiate the model.\n",
    "#mod_doc2vec.build_vocab(training_corpus)\n",
    "#%time model.train(training_corpus, total_examples=mod_doc2vec.corpus_count, epochs=mod_doc2vec.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print len(mod_doc2vec.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 23:18:49,027 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "training_eva_corpus = create_test_train_corpus(patent_file_path, tokens_only=False).next()\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "nr_mod_docs = len(mod_doc2vec.docvecs)\n",
    "for i, doc in enumerate(training_eva_corpus):\n",
    "    if i < 100:\n",
    "        inferred_vector = mod_doc2vec.infer_vector(doc.words)\n",
    "        sims = mod_doc2vec.docvecs.most_similar([inferred_vector], topn=nr_mod_docs)\n",
    "        # get the rank of the same inferred document \n",
    "        rank = [i for i, sim in sims].index(doc[1][0])\n",
    "        ranks.append(rank)\n",
    "        second_ranks.append(sims[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({75: 1,\n",
       "         325: 1,\n",
       "         332: 1,\n",
       "         353: 1,\n",
       "         371: 1,\n",
       "         381: 1,\n",
       "         395: 1,\n",
       "         458: 1,\n",
       "         546: 1,\n",
       "         589: 1,\n",
       "         798: 1,\n",
       "         999: 1,\n",
       "         1122: 1,\n",
       "         1172: 1,\n",
       "         1181: 1,\n",
       "         1285: 1,\n",
       "         1339: 1,\n",
       "         1418: 1,\n",
       "         1564: 1,\n",
       "         1647: 1,\n",
       "         1663: 1,\n",
       "         1674: 1,\n",
       "         1702: 1,\n",
       "         1749: 1,\n",
       "         1867: 1,\n",
       "         1885: 1,\n",
       "         2107: 1,\n",
       "         2134: 1,\n",
       "         2353: 1,\n",
       "         2403: 1,\n",
       "         2622: 1,\n",
       "         2628: 1,\n",
       "         2768: 1,\n",
       "         2990: 1,\n",
       "         3042: 1,\n",
       "         3109: 1,\n",
       "         3110: 1,\n",
       "         3345: 1,\n",
       "         3389: 1,\n",
       "         3487: 1,\n",
       "         3510: 1,\n",
       "         3529: 1,\n",
       "         3679: 1,\n",
       "         3833: 1,\n",
       "         4077: 1,\n",
       "         4240: 1,\n",
       "         4281: 1,\n",
       "         4290: 1,\n",
       "         4324: 1,\n",
       "         4655: 1,\n",
       "         4847: 1,\n",
       "         4966: 1,\n",
       "         4981: 1,\n",
       "         5082: 1,\n",
       "         5150: 1,\n",
       "         5309: 1,\n",
       "         5438: 1,\n",
       "         5627: 1,\n",
       "         5856: 1,\n",
       "         5881: 1,\n",
       "         5924: 1,\n",
       "         5967: 1,\n",
       "         6117: 1,\n",
       "         6534: 1,\n",
       "         6653: 1,\n",
       "         6773: 1,\n",
       "         6781: 1,\n",
       "         6829: 1,\n",
       "         6930: 1,\n",
       "         6991: 1,\n",
       "         7039: 1,\n",
       "         7391: 1,\n",
       "         7462: 1,\n",
       "         7567: 1,\n",
       "         7609: 1,\n",
       "         7713: 1,\n",
       "         7727: 1,\n",
       "         7748: 1,\n",
       "         7753: 1,\n",
       "         7871: 1,\n",
       "         8014: 1,\n",
       "         8174: 1,\n",
       "         8363: 1,\n",
       "         8373: 1,\n",
       "         8389: 1,\n",
       "         8460: 1,\n",
       "         8805: 1,\n",
       "         8827: 1,\n",
       "         8882: 1,\n",
       "         8990: 1,\n",
       "         9053: 1,\n",
       "         9129: 1,\n",
       "         9240: 1,\n",
       "         9306: 1,\n",
       "         9431: 1,\n",
       "         9436: 1,\n",
       "         9464: 1,\n",
       "         9713: 1,\n",
       "         9883: 1,\n",
       "         9894: 1})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================Just testing below this line=====================\n",
    "\n",
    "### Try out Doc2Vec with toy corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 16:36:06,861 : INFO : collecting all words and their counts\n",
      "2017-12-19 16:36:06,863 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-12-19 16:36:06,864 : INFO : collected 11 word types and 3 unique tags from a corpus of 3 examples and 15 words\n",
      "2017-12-19 16:36:06,865 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 16:36:06,866 : INFO : min_count=2 retains 4 unique words (36% of original 11, drops 7)\n",
      "2017-12-19 16:36:06,867 : INFO : min_count=2 leaves 8 word corpus (53% of original 15, drops 7)\n",
      "2017-12-19 16:36:06,868 : INFO : deleting the raw counts dictionary of 11 items\n",
      "2017-12-19 16:36:06,870 : INFO : sample=0.001 downsamples 4 most-common words\n",
      "2017-12-19 16:36:06,871 : INFO : downsampling leaves estimated 0 word corpus (6.7% of prior 8)\n",
      "2017-12-19 16:36:06,872 : INFO : estimated required memory for 4 words and 40 dimensions: 3760 bytes\n",
      "2017-12-19 16:36:06,873 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  [TaggedDocument(words=['I', 'hate', 'debugging'], tags=[0]), TaggedDocument(words=['everyone', 'hate', 'debugging', 'too', 'no', 'joke'], tags=[1]), TaggedDocument(words=['But', 'NLP', 'is', 'FUN', 'no', 'joke'], tags=[2])]\n",
      "Vocabulary: \n",
      "{'debugging': <gensim.models.keyedvectors.Vocab object at 0x7f64c82ebe50>, 'joke': <gensim.models.keyedvectors.Vocab object at 0x7f648807f510>, 'hate': <gensim.models.keyedvectors.Vocab object at 0x7f648807f450>, 'no': <gensim.models.keyedvectors.Vocab object at 0x7f648807f4d0>}\n"
     ]
    }
   ],
   "source": [
    "toy_corpus = [['I','hate','debugging'],\n",
    "               ['everyone','hate','debugging','too','no','joke'],\n",
    "               ['But','NLP','is','FUN', 'no', 'joke']]\n",
    "\n",
    "def generate_test_corpus(corpus):\n",
    "    for i, doc in enumerate(corpus):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(doc, [i])\n",
    "\n",
    "tagged_corpus = list(generate_test_corpus(toy_corpus))\n",
    "print \"Corpus: \", tagged_corpus\n",
    "\n",
    "test_model = gensim.models.doc2vec.Doc2Vec(size=40, min_count=2, iter=5)\n",
    "test_model.build_vocab(tagged_corpus)\n",
    "\n",
    "print \"Vocabulary: \"\n",
    "print test_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0074688 , -0.00860952,  0.00651344,  0.0067819 , -0.00940804,\n",
       "       -0.00573374,  0.00446753,  0.01036623,  0.01099162, -0.00230265,\n",
       "        0.01001389,  0.00422939, -0.00035249,  0.00650207,  0.00496055,\n",
       "        0.01139371, -0.00698338, -0.00481543,  0.00873296,  0.00184248,\n",
       "       -0.01131732, -0.00826996,  0.00937591, -0.0049616 , -0.00500902,\n",
       "        0.01158319,  0.00515296, -0.00621486, -0.00086252, -0.00193153,\n",
       "        0.00424253, -0.00150474, -0.00628895, -0.00585909, -0.01022338,\n",
       "       -0.00081373,  0.00304008,  0.0053918 ,  0.00058929,  0.01156284], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test infer vector on a new doc\n",
    "test_model.infer_vector(['Only', 'I', 'can', 'dealt', 'with', 'debugging', 'no', 'joke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
